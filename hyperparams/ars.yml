CartPole-v1:
  n_envs: 2
  n_timesteps: !!float 2e5
  policy: 'LinearPolicy'

Pendulum-v0:
  n_envs: 1
  n_timesteps: !!float 5e6
  policy: 'MlpPolicy'
  normalize: "dict(norm_obs=True, norm_reward=False)"
  learning_rate: !!float 0.018
  n_delta: 4
  n_top: 1
  delta_std: 0.1
  policy_kwargs: "dict(net_arch=[16])"

LunarLander-v2:
  n_envs: 1
  n_timesteps: !!float 1e6
  policy: 'LinearPolicy'
  normalize: "dict(norm_obs=True, norm_reward=False)"


LunarLanderContinuous-v2:
  normalize: true
  n_timesteps: !!float 1e6
  policy: 'LinearPolicy'

Acrobot-v1:
  normalize: true
  n_envs: 2
  n_timesteps: !!float 1e6
  policy: 'LinearPolicy'

MountainCar-v0:
  normalize: true
  n_envs: 4
  n_timesteps: !!float 1e6
  policy: 'LinearPolicy'

MountainCarContinuous-v0:
  normalize: True
  n_envs: 4
  n_timesteps: !!float 1e6
  policy: 'LinearPolicy'
  n_delta: 16

# === Pybullet Envs ===

# TO be tuned
HalfCheetahBulletEnv-v0: &pybullet-defaults
  normalize: "dict(norm_obs=True, norm_reward=False)"
  n_envs: 1
  policy: 'LinearPolicy'
  n_timesteps: !!float 1.25e7
  learning_rate: !!float 0.02
  delta_std: !!float 0.03
  n_delta: 32
  n_top: 4
  alive_bonus_offset: 0
  normalize: "dict(norm_obs=True, norm_reward=False)"

AntBulletEnv-v0:
  n_envs: 1
  policy: 'LinearPolicy'
  n_timesteps: !!float 7.5e7
  learning_rate: !!float 0.015
  delta_std: !!float 0.025
  n_delta: 60
  n_top: 20
  alive_bonus_offset: -1
  normalize: "dict(norm_obs=True, norm_reward=False)"

Walker2DBulletEnv-v0:
  <<: *pybullet-defaults
  policy: 'LinearPolicy'
  n_timesteps: !!float 7.5e7
  learning_rate: !!float 0.03
  delta_std: !!float 0.025
  n_delta: 40
  n_top: 30
  alive_bonus_offset: -1
  normalize: "dict(norm_obs=True, norm_reward=False)"

HopperBulletEnv-v0:
  n_envs: 1
  policy: 'LinearPolicy'
  n_timesteps: !!float 7e6
  learning_rate: !!float 0.01
  delta_std: !!float 0.025
  n_delta: 8
  n_top: 4
  alive_bonus_offset: -1
  normalize: "dict(norm_obs=True, norm_reward=False)"

ReacherBulletEnv-v0:
  <<: *pybullet-defaults
  n_timesteps: !!float 1e6

# === Mujoco Envs ===
# Params closest to original paper
Swimmer-v3:
  n_envs: 1
  policy: 'LinearPolicy'
  n_timesteps: !!float 2e6
  learning_rate: !!float 0.02
  delta_std: !!float 0.01
  n_delta: 1
  n_top: 1
  alive_bonus_offset: 0
  # normalize: "dict(norm_obs=True, norm_reward=False)"

Hopper-v3:
  n_envs: 1
  policy: 'LinearPolicy'
  n_timesteps: !!float 7e6
  learning_rate: !!float 0.01
  delta_std: !!float 0.025
  n_delta: 8
  n_top: 4
  alive_bonus_offset: -1
  normalize: "dict(norm_obs=True, norm_reward=False)"

HalfCheetah-v3:
  n_envs: 1
  policy: 'LinearPolicy'
  n_timesteps: !!float 1.25e7
  learning_rate: !!float 0.02
  delta_std: !!float 0.03
  n_delta: 32
  n_top: 4
  alive_bonus_offset: 0
  normalize: "dict(norm_obs=True, norm_reward=False)"

Walker2d-v3:
  n_envs: 1
  policy: 'LinearPolicy'
  n_timesteps: !!float 7.5e7
  learning_rate: !!float 0.03
  delta_std: !!float 0.025
  n_delta: 40
  n_top: 30
  alive_bonus_offset: -1
  normalize: "dict(norm_obs=True, norm_reward=False)"

Ant-v3:
  n_envs: 1
  policy: 'LinearPolicy'
  n_timesteps: !!float 7.5e7
  learning_rate: !!float 0.015
  delta_std: !!float 0.025
  n_delta: 60
  n_top: 20
  alive_bonus_offset: -1
  normalize: "dict(norm_obs=True, norm_reward=False)"


Humanoid-v3:
  n_envs: 1
  policy: 'LinearPolicy'
  n_timesteps: !!float 2.5e8
  learning_rate: 0.02
  delta_std: 0.0075
  n_delta: 256
  n_top: 256
  alive_bonus_offset: -5
  normalize: "dict(norm_obs=True, norm_reward=False)"


BipedalWalker-v3:
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e8
  learning_rate: 0.02
  delta_std: 0.0075
  n_delta: 64
  n_top: 32
  alive_bonus_offset: -0.1
  normalize: "dict(norm_obs=True, norm_reward=False)"
  policy_kwargs: "dict(net_arch=[16])"

# TO Be Tuned
BipedalWalkerHardcore-v3:
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 5e8
  learning_rate: 0.02
  delta_std: 0.0075
  n_delta: 64
  n_top: 32
  alive_bonus_offset: -0.1
  normalize: "dict(norm_obs=True, norm_reward=False)"
  policy_kwargs: "dict(net_arch=[16])"
